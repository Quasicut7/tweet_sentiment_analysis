{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bb21cf",
   "metadata": {},
   "source": [
    "### Imports and Setups\n",
    "import emoji (This module is used to remove emojis from text but the emojis are already stripped from the Sentiment140 dataset so it is not used in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42609ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee14e1b",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "I chose Sentiment140 Tweet dataset becuase it contains 1.6 M labeled tweets, and is a good example to test machine learning skills on high amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"database//tweets.csv\", names = ['sentiment', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575c1a2",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "0 is for negative and 4 is for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts().plot(kind = 'bar', color = ['red','green'])\n",
    "plt.title(\"Sentiment Distribution (Raw Data)\")\n",
    "plt.xlabel(\"Sentiment (0 = negative, 4 = positive)\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8881486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['text'].apply(len)\n",
    "\n",
    "plt.hist(df['tweet_length'], bins = 50, color = 'skyblue', edgecolor = 'black')\n",
    "plt.title(\"Tweet Length Distribution (Raw Data)\")\n",
    "plt.xlabel(\"Length (characters)\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adab5bc",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef68411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(string):\n",
    "    string = re.sub(r\"http\\S+|www\\S+\", \"\", string)\n",
    "    string = re.sub(r\"@\\w+\", \"[USER]\", string)\n",
    "    string = re.sub(r\"#\", \"\", string)\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    string = string.lower().strip()\n",
    "    return string\n",
    "\n",
    "df[\"processed_text\"] = df[\"text\"].apply(cleaner)\n",
    "df[[\"text\", \"processed_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61463c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(string):\n",
    "    words = [lemmatizer.lemmatize(word) for word in string.split()]\n",
    "    result = ' '.join(words)\n",
    "    return result\n",
    "\n",
    "df[\"lemmatized_text\"] = df[\"processed_text\"].apply(lemmatize)\n",
    "df[[\"text\", \"lemmatized_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a4733",
   "metadata": {},
   "source": [
    "### Train Test Data Split\n",
    "The data is split in train and test parts in ratio 9:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ed958",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"lemmatized_text\"]\n",
    "y = df[\"sentiment\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62422b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 10, stratify = y)\n",
    "print(\"X Train: \", x_train.shape)\n",
    "print(\"Y Train: \", y_train.shape)\n",
    "print(\"X Test: \", x_test.shape)\n",
    "print(\"Y Test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48354d3c",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Training\n",
    "This model has limitation that it doesn't understand sarcasm and contextual text due to use of TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 20000, ngram_range = (1,2))\n",
    "x_train_int = vectorizer.fit_transform(x_train)\n",
    "x_test_int = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 500, C = 0.5)\n",
    "model.fit(x_train_int, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5e4a7",
   "metadata": {},
   "source": [
    "### Evaluating Model\n",
    "Changed model's accuracy by setting hyperparameter, number of iterations and max number of features for vectorizer resulting in the model's accuracy to 78.39% which is very accurate for such a big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test_int)\n",
    "\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save model and vectorizer\n",
    "with open('sentiment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print('Model and vectorizer saved successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
